name: Llama Runtime Test

on:
  push:
    branches: [ "main", "*dev", "ci*" ]
  pull_request:
    branches: [ "main", "*dev" ]
  workflow_dispatch:

jobs:
  runtime-test:
    name: Runtime Test
    runs-on: ${{ matrix.os }}
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
        features: [cuda, metal]
        exclude:
          # Metal only works on macOS
          - os: ubuntu-latest
            features: metal
          # CUDA only works on Ubuntu (for CI)
          - os: macos-latest
            features: cuda

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
        components: rustfmt, clippy

    - name: Check for GPU hardware
      id: gpu-check
      if: matrix.os == 'ubuntu-latest' && matrix.features == 'cuda'
      run: |
        if command -v nvidia-smi &> /dev/null; then
          if nvidia-smi &> /dev/null; then
            echo "gpu_available=true" >> $GITHUB_OUTPUT
            echo "âœ… GPU detected:"
            nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
          else
            echo "gpu_available=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  nvidia-smi found but no GPU detected"
          fi
        else
          echo "gpu_available=false" >> $GITHUB_OUTPUT
          echo "â„¹ï¸  No GPU detected, skipping CUDA tests"
        fi

    - name: Install CUDA and NVIDIA drivers (Ubuntu)
      if: matrix.os == 'ubuntu-latest' && matrix.features == 'cuda' && steps.gpu-check.outputs.gpu_available == 'true'
      run: |
        chmod +x ./installers/setup-nvidia.sh
        ./installers/setup-nvidia.sh

    - name: Setup Rust cache
      uses: Swatinem/rust-cache@v2

    - name: Cache model files
      uses: actions/cache@v3
      with:
        path: examples/llama/setup
        key: llama-models-${{ hashFiles('examples/llama/setup/setup.sh') }}

    - name: Run setup script if needed
      working-directory: examples/llama
      run: |
        if [ "${{ matrix.features }}" == "cuda" ] && [ "${{ steps.gpu-check.outputs.gpu_available }}" != "true" ]; then
          echo "â­ï¸ Skipping model setup - CUDA test will be skipped"
          exit 0
        fi
        if [ ! -f setup/*.gguf ]; then
          bash ./setup/setup.sh
        else
          echo "Model files already exist, skipping setup"
        fi

    - name: Run llama example  
      id: llama-test
      working-directory: examples/llama
      timeout-minutes: 20
      env:
        LD_LIBRARY_PATH: "/usr/local/cuda-12.2/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
      run: |
        if [ "${{ matrix.features }}" == "cuda" ] && [ "${{ steps.gpu-check.outputs.gpu_available }}" != "true" ]; then
          echo "â­ï¸ Skipping CUDA test - no physical GPU available"
          echo "conclusion=skipped" >> $GITHUB_OUTPUT
          exit 0
        fi
        echo "ðŸš€ Running Llama model with ${{ matrix.features }}..."
        if [ "${{ steps.gpu-check.outputs.gpu_available }}" == "true" ]; then
          echo "âœ… Using physical GPU acceleration"
        else
          echo "âœ… Using ${{ matrix.features }} acceleration"
        fi
        cargo run --release --features ${{ matrix.features }}
        echo "conclusion=success" >> $GITHUB_OUTPUT
