name: Llama Runtime Test

# Tests the Llama example with model download and execution
# - Multi-platform: Ubuntu (CPU/default), macOS (CPU/Metal)
# - CUDA testing: Only on GPU runners when explicitly requested or workflow_dispatch
# - Includes model caching and setup script execution

on:
  push:
    branches: [ "main", "*dev", "ci*" ]
  pull_request:
    branches: [ "main", "*dev" ]
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always

jobs:
  detect-gpu:
    name: GPU Detection
    runs-on: ubuntu-latest
    outputs:
      cuda_available: ${{ steps.gpu.outputs.available }}
    steps:
    - name: Check for NVIDIA GPU
      id: gpu
      run: |
        if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
          echo "available=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Physical GPU detected"
          nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
        else
          echo "available=false" >> $GITHUB_OUTPUT
          echo "‚ÑπÔ∏è No physical GPU detected"
        fi

  multi-platform-test:
    name: Multi-Platform Test
    runs-on: ${{ matrix.os }}
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
        features: [default, metal]
        exclude:
          # Metal only works on macOS
          - os: ubuntu-latest
            features: metal

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache Cargo
      uses: Swatinem/rust-cache@v2
      with:
        save-if: always()

    - name: Cache model files
      uses: actions/cache@v4
      with:
        path: examples/llama/setup
        key: llama-models-${{ hashFiles('examples/llama/setup/setup.sh') }}

    - name: Run setup script if needed
      working-directory: examples/llama
      run: |
        if [ ! -f setup/*.gguf ]; then
          bash ./setup/setup.sh
        else
          echo "Model files already exist, skipping setup"
        fi

    - name: Run llama example
      working-directory: examples/llama
      timeout-minutes: 15
      env:
        LD_LIBRARY_PATH: "/usr/local/cuda-12.2/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
      run: |
        echo "üöÄ Running Llama model with ${{ matrix.features }}..."
        if [[ "${{ matrix.features }}" == "default" ]]; then
          echo "‚úÖ Using CPU backend"
          cargo run --release
        else
          echo "‚úÖ Using ${{ matrix.features }} backend"
          cargo run --release --features ${{ matrix.features }}
        fi

  cuda-test:
    name: CUDA Test
    runs-on: ${{ github.repository_owner == 'lucky-tensor' && 'gpu-runners' || 'ubuntu-latest' }}
    needs: detect-gpu
    timeout-minutes: 60
    # Only run CUDA tests when explicitly requested via 'test-gpu' label or manual trigger
    if: contains(github.event.pull_request.labels.*.name, 'test-gpu') || github.event_name == 'workflow_dispatch'

    env:
      CUDA_AVAILABLE: ${{ needs.detect-gpu.outputs.cuda_available }}

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Setup CUDA
      if: env.CUDA_AVAILABLE != 'true'
      run: |
        chmod +x ./installers/setup-nvidia.sh
        ./installers/setup-nvidia.sh

    - name: Cache Cargo
      uses: Swatinem/rust-cache@v2
      with:
        save-if: always()

    - name: Cache model files
      uses: actions/cache@v4
      with:
        path: examples/llama/setup
        key: llama-models-${{ hashFiles('examples/llama/setup/setup.sh') }}

    - name: Run setup script if needed
      working-directory: examples/llama
      run: |
        if [[ "${{ env.CUDA_AVAILABLE }}" != "true" ]]; then
          echo "‚è≠Ô∏è Skipping model setup - CUDA test will be skipped"
          exit 0
        fi
        if [ ! -f setup/*.gguf ]; then
          bash ./setup/setup.sh
        else
          echo "Model files already exist, skipping setup"
        fi

    - name: Run llama example with CUDA
      working-directory: examples/llama
      timeout-minutes: 15
      env:
        LD_LIBRARY_PATH: "/usr/local/cuda-12.2/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
      run: |
        if [[ "${{ env.CUDA_AVAILABLE }}" != "true" ]]; then
          echo "‚è≠Ô∏è Skipping CUDA test - no physical GPU available"
          exit 0
        fi
        
        echo "üöÄ Running Llama model with CUDA..."
        echo "‚úÖ Using physical GPU acceleration"
        cargo run --release --features cuda