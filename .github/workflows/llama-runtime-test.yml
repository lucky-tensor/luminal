name: Llama Runtime Test

on:
  push:
    branches: [ "main", "*dev", "ci*" ]
  pull_request:
    branches: [ "main", "*dev" ]
  workflow_dispatch:

jobs:
  runtime-test:
    name: Runtime Test
    runs-on: ${{ matrix.os }}
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
        features: [cuda, metal]
        exclude:
          # Metal only works on macOS
          - os: ubuntu-latest
            features: metal
          # CUDA only works on Ubuntu (for CI)
          - os: macos-latest
            features: cuda

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
        components: rustfmt, clippy

    - name: Install NVIDIA drivers and setup paths (Ubuntu)
      if: matrix.os == 'ubuntu-latest' && matrix.features == 'cuda'
      run: |
        # Install NVIDIA drivers
        sudo apt-get update
        sudo apt-get install -y nvidia-driver-535 nvidia-utils-535
        
        # Set up library paths
        echo "/usr/local/cuda/lib64" | sudo tee -a /etc/ld.so.conf
        echo "/usr/lib/x86_64-linux-gnu" | sudo tee -a /etc/ld.so.conf
        sudo ldconfig
        
        # Export library path for this session
        export LD_LIBRARY_PATH="/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
        
        # Verify installation
        nvidia-smi || echo "nvidia-smi not available"
        ls -la /usr/lib/x86_64-linux-gnu/libcuda* || echo "libcuda not found"

    - name: Setup Rust cache
      uses: Swatinem/rust-cache@v2

    - name: Cache model files
      uses: actions/cache@v3
      with:
        path: examples/llama/setup
        key: llama-models-${{ hashFiles('examples/llama/setup/setup.sh') }}

    - name: Run setup script if needed
      working-directory: examples/llama
      run: |
        if [ ! -f setup/*.gguf ]; then
          bash ./setup/setup.sh
        else
          echo "Model files already exist, skipping setup"
        fi

    - name: Run llama example
      working-directory: examples/llama
      env:
        LD_LIBRARY_PATH: "/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
      run: cargo run --release --features ${{ matrix.features }}
