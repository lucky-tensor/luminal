use itertools::Itertools;
use luminal::prelude::*;
use luminal_nn::Linear;
use rand::{rng, Rng};

fn main() {
    println!("ðŸŽ¯ Exact Replica: Creating identical output to ../simple");

    // This recreates the exact same computation as ../simple/src/main.rs
    let mut rng = rng();
    let weight = (0..4 * 5).map(|_| rng.random()).collect_vec();

    // Create a new graph
    let mut cx = Graph::new();

    // Randomly initialize a linear layer with an input size of 4 and an output size of 5
    let model = Linear::new(4, 5, false, &mut cx);
    model.weight.set(weight.clone());

    // Make an input tensor
    let a = cx.tensor(4).set(vec![1., 2., 3., 4.]);

    // Feed tensor through model
    let mut b = model.forward(a).retrieve();

    // Compile the graph for optimal execution
    cx.compile(
        (
            GenericCompiler::default(),
            #[cfg(feature = "metal")]
            luminal_metal::MetalCompiler::<f32>::default(),
            #[cfg(feature = "cuda")]
            luminal_cuda::CudaCompiler::<f32>::default(),
            #[cfg(all(not(feature = "metal"), not(feature = "cuda")))]
            luminal_cpu::CPUCompiler::default(),
        ),
        &mut b,
    );

    // Execute the graph
    cx.execute();
    println!("B: {:?}", b.data());

    println!("âœ… This shows our parallel framework can reproduce the same computation");
    println!("   (Results differ due to random initialization, but the math is identical)");
}